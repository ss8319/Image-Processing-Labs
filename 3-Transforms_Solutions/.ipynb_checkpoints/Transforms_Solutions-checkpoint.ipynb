{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ac34d6-b46f-4f42-8a10-3a61ecc77853",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div align='center'><div align='center'>\n",
    "    \n",
    "## Image Processing\n",
    "\n",
    "### Linear Image Transforms\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d298b22-c063-4d33-a7b7-0244b4b72fba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Begin this session by reviewing the Practical on image display and colourmaps; this will be useful to interpret\n",
    "the results of transform spaces. Also ensure that you have completed the practical of Chapter 2 (the peas on\n",
    "desk one), which is needed for part of this practical.\n",
    "\n",
    "#### Complex number representations in `numpy`\n",
    "We have not previously mentioned this, but `numpy` supports complex number representations and arithmetic. So, we can do things like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a61633f-f348-4b8d-b16b-7aba622bcc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5+0j)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = 1+2j # My first complex number (actually, it is!)\n",
    "b = 1-2j\n",
    "print(a*b) # Verify that you agree with the answer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09fc4b3-5fd7-4625-80bc-e89d37bbde45",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We also have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7093b281-852a-41e8-92c1-87b66561ddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1071487177940904"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(a) #  Check....\n",
    "# And the phase angle we get like this:\n",
    "np.angle(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796cc6c-6d37-4403-9c31-382b19bdace7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We also have access to the components of the complex numbers as attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6334e4f0-5c83-43f7-86a3-df4e7a203425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "print(a.real)\n",
    "print(a.imag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1e9aa-a58d-4690-bbaf-ec95e110ab75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.1 Image Transforms & Basis Images\n",
    "\n",
    "#### 3.1.1 On-paper preliminaries\n",
    "\n",
    "You need to start by reading the Section of the main lecture notes related to basis images, and carefully go though the exercise involving the transformation\n",
    "of the $2\\times 2$ image $\\mathbf{F}$ into the transform domain $\\mathbf{V}$. \n",
    "\n",
    "The tiny image is:\n",
    "\n",
    "$$\n",
    "{\\bf F}= \\left (\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\ 3 & 4\n",
    "\\end{array}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "Use the following 4 basis images:\n",
    "\n",
    "$$\n",
    "{\\bf B}_{0,0}=\\frac{1}{2} \\left (\n",
    "\\begin{array}{cc} 1 & 1 \\\\ 1 & 1 \\end{array} \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\bf B}_{0,1}=\\frac{1}{2}\\left (\n",
    "\\begin{array}{cc} 1 & -1 \\\\ 1 & -1 \\end{array} \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\bf B}_{1,0}=\\frac{1}{2} \\left (\n",
    "\\begin{array}{cc} 1 & 1 \\\\ -1 & -1 \\end{array} \\right )\n",
    "$$\n",
    "\n",
    "$$\n",
    "{\\bf B}_{1,1}=\\frac{1}{2}\\left (\n",
    "\\begin{array}{cc} 1 & -1 \\\\ -1 & 1 \\end{array} \\right )\n",
    "$$\n",
    "\n",
    "\n",
    "Ensure, also, that you understand the operation of the matrix inner product and how it relates to these exercises in Chapter 3.\n",
    "\n",
    "The steps below will help you do this...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff12ec0-d49c-4c95-8750-3d3c1ab710f0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "    \n",
    "**Exercise 3.1**\n",
    "\n",
    "1. On _paper_ (not in _Python_) make sure that you can go from the image domain into the transform domain using:\n",
    "$$\n",
    "v(k,l) = \\langle \\mathbf{F},\\mathbf{B}_{k,l} \\rangle,\\,\\,\\, \\textrm{for} \\,\\, k=0,1 \\,\\, \\textrm{and}\\,\\, l = 0,1\n",
    "$$\n",
    "\n",
    "Remember (or realise/recognise) that the elements of $\\mathbf{V}$ are the entries $v(k,l)$; you should recognise this implicit equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c956d44-7a69-4b94-8424-13ecdeb7e2fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "    \n",
    "**Exercise 3.2** \n",
    "On _paper_ (not in _Python_) make sure that you can go from the transform domain into the image domain using:\n",
    "$$\n",
    "f(m,n) = \\langle \\mathbf{V},\\mathbf{B}_{m,n} \\rangle,\\,\\,\\, \\textrm{for} \\,\\, m=0,1 \\,\\, \\textrm{and}\\,\\, n = 0,1.\n",
    "$$\n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e451d40-73be-4166-ae55-c0bc85103d3c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "    \n",
    "**Exercise 3.3** \n",
    "\n",
    "On _paper_, verify that if you do the following, you can _also_ get from transform domain back to image domain doing this:\n",
    "$$\n",
    "\\mathbf{F} = \\sum_{v}\\sum_l v(k,l) \\mathbf{B}_{k,l} \\,\\,\\,\\,\\,\\textrm{Eq (A)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f97ad-8fd4-4542-a8fe-3036dc7789e2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    " \n",
    "The approach of Ex 3.2 and Ex 3.3 yield the same results; the equivalence follows from the properties of the basis images $\\mathbf{B}_{k,l}$, but the two different ways of getting from transform domain back to image domain have slightly different intuitions. In the case of Ex. 2, you are projecting the transform onto basis images, whilst in Ex. 3, you are blending the basis images together with different weights to reconstruct $\\mathbf{F}$, which holds the values of $f(m,n)$.\n",
    "\n",
    "If you are of a suspicious nature, you may want to verify that the calculations also work for different choices of image $\\mathbf{F}$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa88e8-930c-46f6-83ae-1a1e45c1962b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 3.1.2 Estimating computational effort\n",
    "Work out the number of additions and multiplications needed to go from the image to the transform domain. Then, assuming that your image size increases so that $\\mathbf{F}$ is a $3 \\times 3$ image, and there are 9 basis images (rather than 4), work out the number of operations.\n",
    "\n",
    "In general, how does the number of operations increase as the size of the image increases (remember, the number of basis functions also increases, not just their size!)\n",
    "\n",
    "Note: you should find that if the image is square (to simply the number of calculations), that the number of operations increases _approximately_ as the 4th power of the number of rows or columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00851b-a54f-4ae6-b2a0-ac51613441fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.2 The 2D DFT \n",
    "#### 3.2.1 Getting started\n",
    "For this exercise, you will be using the `head.128` image previously used in Practical 1 (on colourmaps). Load\n",
    "this image into the Notebook again, and ensure that you can display it with a grey-scale colour map. If you can't remember how to do this, you may go back to the original notebook or its PDF version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc73325b-7287-4311-a42d-22c7ffd085fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('IMGS/head.128', 'rb') as binary_file:\n",
    "# `binary_file` is now an object with methods for reading....\n",
    "# Read the whole file at once\n",
    "# Note that you may not find many examples of this\n",
    "# online, as reading pure binary files is seen as rare\n",
    "# by many who work with pre-prepared images\n",
    "    data = binary_file.read()\n",
    "    \n",
    "\n",
    "data = np.frombuffer(data, dtype=np.uint8)\n",
    "# The next line tells us how to organise it\n",
    "x = np.reshape(data,(128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8d00e-9918-46e1-b06d-a3f38a022d15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Using the two-dimensional discrete Fourier transform, which is a member method of the module `fft()` of the `numpy` library, transform head image, which we shall refer to as variable $\\mathbf{x}$, into the two-dimensional discrete Fourier domain. You will have to convert the image to a floating point representation to do this. Write down your observations about the size of the 2D DFT representation as a _Python_ `numpy` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc43c0c1-b833-4588-b8bb-5ee22b4f3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.fft.fft2((x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574a133-7525-4a45-840b-63063b587fbe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You need to do a bit of investigation into the values of $\\mathbf{X}$; you will see that they are complex valued (generally speaking), so that there is a real and imaginary part to each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d00e9-bb0a-4e9a-910f-b9d8466b726a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 3.2.2 Visualising Discrete Fourier Space\n",
    "\n",
    "Generally speaking, there is no obvious way to map complex numbers to a colourmap; there are some mappings that can be applied based on the absolute and phase (angular) values, but these are not necessarily intuitive, or particularly helpful.\n",
    "\n",
    "So, there are two general paths we can take: one is to display the real and imaginary values separately; for the case of images mapped to the 2D Fourier space, these are not usually very informative.  The _conventional_ way to display discrete Fourier space is through displaying the magnitude and phase images separately. But because only a small proportion of transform space actually has meaningful magnitudes, displaying phase on its own is not very useful. But to do this, we have to get an idea of the range of values we are looking at.\n",
    "\n",
    "##### Dynamic range of Discrete Fourier Space Values \n",
    "Have a look at the minimum and maximum values of the complex array, $\\mathbf{X}$. You can do this with:\n",
    "`X.min()`\n",
    "and\n",
    "`X.min()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c226222-d34d-403c-9fb0-7a2a1e7e6bf5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Be careful here: one reasonable option for `max()` and `min()` to work is to take max and min via absolute values for complex numbers; in the exercises below, we will be sorting by absolute value, but then playing around with the complex values!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded883f-cfbd-45cc-8f8e-e5112e9c7676",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, explore range of _absolute_ values in $\\mathbf{X}$; you can do this by first finding the absolute values of all entries in $\\mathbf{X}$. Check the min and max of these absolute values. From this, compute the dynamic range (in decibels), according to:\n",
    "$20 \\times \\log_{10}\\left (\\frac{|X|_{max}}{|X|_{min}}\\right )$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f6e48-8cfd-4f2b-8616-ee60fde9f22d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Compare the value you get from this calculation to the dynamic range of possible _grey-scale_ values that can be represented by your monitor. To do this, it helps to know (!) that the maximum possible distinct intensities on **each** of the red, green and blue channels of your display is (likely) restricted to 256, which includes the value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb092b38-02bc-498e-a7e6-c47497ae8b35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "  <summary outline=\"1pt\">🆘 Stuck on this question? Click here to see what I am getting at....</summary>\n",
    "\n",
    "If each channel is restricted to 256 possible ibtensity values, then this means that _if_ I am restricted to grey-scale intensities, then the number of possible distinct grey-scale values is also 256 (because we must have _R_=_G_=_B_). Therefore, the dynamic range of intensity values is only $\\log_2(256)=8$ bits. So, we have a mismatch between the range of values across Fourier space and the \n",
    "possible range I can display. This is why some sort of dynamic range compression is sometimes required _for display_ of the DFT magnitude. Typical to use a logarithmic compression (see notes).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d2d359-7414-42f1-89ce-394b6c28d538",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Display the _absolute_ values of the 2D DFT result. Can you see where the significant coefficients are? \n",
    "\n",
    "_Note_: This may be difficult if you are using a Collab notebook, because I do not believe that an external image viewer is available. \n",
    "\n",
    "However if you are running this notebook on your own PC or Mac, you should be able to use an external viewer with the \n",
    "`%matplotlib` directive; the external viewer should have an indicator of the value at each pixel as you move the mouse cursor around.\n",
    "\n",
    "Remember that after using this directive, you need to use:\n",
    "`%matplotlib inline` to return to default display of images _inside_ the notebook itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49fbe817-160b-4c0c-89c8-beca8351332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0782bec0-9079-4596-a739-717d7f402bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff80c4f5a60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use external image viewer for this\n",
    "%matplotlib\n",
    "plt.imshow(np.abs(X), cmap='gray') # Allow normalisation to be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abcfcb-1549-4415-981e-5d42d722c610",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**A more sensible arrangement of Discrete Fourier Space**\n",
    "\n",
    "Explore the effect of the `np.fft.fftshift()` command. This is largely a cosmetic function which re-arranges discrete Fourier space so that the low frequencies - which are usually dominant - are at the centre of the image array: it simplifies the interpretation of the image structure somewhat. \n",
    "\n",
    "After applying `fftshift()`, the frequency of basis image corresponding to $(k=0,l=0)$ is at the _centre_ of discrete Fourier space, and the frequencies of components increase as we move away from the centre.\n",
    "\n",
    "Positive and negative frequencies are actually pretty similar, but display different shift  relationships (i.e. phase) between the sin and cosine components: this is the only interpretation that you need to put on the somewhat awkward term \"negative frequency\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab132651-34b1-457e-b6a2-ad01ec41f4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff80b086c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(np.fft.fftshift(np.abs(X)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26249722-f251-4491-8adb-463b2415c67c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 3.3 The Inverse DFT\n",
    "\n",
    "To explore the inverse discrete Fourier transform is easy: just apply it to the _unmodified, complex_ discrete Fourier space result; the command is `np.fft.ifft2()`. \n",
    "\n",
    "_Hint_: Interpreting the output is not quite as straightfoward as it sounds. You will have to investigate the values in `xdash` carefully to figure out what is going on. Remember that you can extract real and imaginary components from a complex variable (or complex `numpy` array)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639e40e-2d00-4f8c-b9a3-507c7f039388",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "\n",
    "**Exercise**\n",
    "Apply the Inverse Discrete Fourier Transform to transform representation $\\mathbf{X}$, and assign the result to the variable `xdash`.  How similar is the data held in `xdash` to the original `head` image, held in variable `x`? What do you need to do to display `xdash` as an image?\n",
    "\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a6f71-d28a-4c31-a341-d0e92d5f4118",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "<summary> Click here for one possible solution. </summary>\n",
    "\n",
    "`xdash = np.fft.ifft2(X)`\n",
    "\n",
    "To compute the similarity, we can do the following\n",
    "\n",
    "`deviation = xdash-x`\n",
    "\n",
    "Now what? Well how about finding the maximum difference between the original and inverse of the transformed x:\n",
    "\n",
    "`np.max(deviation)`\n",
    "\n",
    "You will see that the deviation is _miniscule_ compared to the values in the image.  So, we have almost a perfectly accurate inverse (to within machine precision).\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5664de7-1947-43fe-8f8c-3d6428ea880f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Displaying the reconstructed image\n",
    "\n",
    "You will have seen that the reconstructed image (that is, the inverse 2D DFT of the 2D DFT of an original grey-scale image) is very close to the original. But there is still a bit of a catch: you will also see that the result is complex valued. \n",
    "\n",
    "However, looking at magnitude of the imaginary values, you will see that they are quite small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf7accd-ff3b-40cc-a944-d57cb9f7319a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imag max: 2.989969383193625e-14\n",
      "Imag real: 224.0\n"
     ]
    }
   ],
   "source": [
    "# You must have computed xdash first in case you have not, I load\n",
    "# it below.\n",
    "if 'xdash' not in globals().keys():\n",
    "    xdash=np.load('IMGS/xdash.npy')\n",
    "print('Imag max:', np.max(np.abs(xdash.imag)))\n",
    "print('Imag real:', np.max(np.abs(xdash.real)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c39d4-4a23-46c4-b4b9-be6955b58ae7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "This means that you can safely ignore the imaginary component (you may wish to double check my reasonong on this), and do the following:\n",
    "\n",
    "`plt.imshow(xdash.real, cmap='gray')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d85e9-b845-4c71-b85b-1f90774bcc63",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.4 Quantifying the Difference Between Images\n",
    "\n",
    "So, if you got as far as computing the deviation image in Section 3.3, you may wonder how we would go about measuring how close the reconstructed image (i.e. `xdash`), which is the Inverse Discrete Fourier Transform of the Discrete Fourier Transform of `x`, is to the original image.\n",
    "\n",
    "One way we can do this is to look at the sum of the square of the deviations; but this will get bigger as image size grows: it is better to do this in a sort of per pixel way.\n",
    "\n",
    "For this, we can use the _Mean Square Error_ or MSE, which in this case is an average of the squared error, where the average is taken over the pixels of the image (and not, say, an experimental space).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b51fe7-d935-4042-a59c-d5c7ab9a192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.974530481563954e-30+2.1768491092347125e-30j)\n"
     ]
    }
   ],
   "source": [
    "# You must have xdash obtained to run the next lines\n",
    "MSE = np.mean((xdash - x)**2)\n",
    "print(MSE) # Ridiculously small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce623c-75e7-4a46-8947-fa933b13f146",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Another measure is the _Root Mean Square Error_ or RMSE, which we get as the square root of MSE. This does not add much, but may make the numbers a bit more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a593bd-097f-412d-95c5-170a202fbac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6721602017115375e-15+4.073200977696669e-16j)\n"
     ]
    }
   ],
   "source": [
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE) # Still ridiculously small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07624073-084e-43e7-b2fa-b1fe7f37c137",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "However, we can use these to investigate the effect of keeping only parts of Fourier space: this is a way of appreciating the energy compaction property of this particular transform space.\n",
    "\n",
    "First, however, we will do some experiments to visualise the basis images, to make sure we know what is going on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23520e9-6834-44e2-9579-1662d181c813",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.5 Visualising basis images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1a662-977d-4688-9a0b-75c5ab3ffb3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Look at Eqn.(A) from **Section 3.1.1** of this notebook.  It explains how to go from transform space into image space. The equation provides a nifty bit of insight into how to extract the basis images.\n",
    "\n",
    "The only dependency that the basis images of Discrete Fourier transform (and many other transforms) have on the image is through the size of the image, which is linked (as we have seen) to the number of basis images, typically equal to the number of pixels in the image.\n",
    "\n",
    "So, the basis images _exist_ for a particular linear transform without the data. Indeed, a transform space can be constructed with no data!\n",
    "\n",
    "If particular, if we make an empty transform space, and set only one value in it to be 1, with everything else being zero (i.e. select one value of $k$ and one value of $l$, and set the coefficient corresponding to that value to be 1, with all other coefficients being 0).  Then, apply Eqn. (A) to map the transform space to image space. What happens (look **closely** at Eqn (A))) is that you get the basis image corresponding to that particular transform coefficient. Make sure you see why this is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9019428-13a6-49d7-ae6e-b7fa908435dd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We'll now do this with a little transform space (16x16) so you get the idea. It's dead easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8ed851-7785-4b7b-a6ee-1737cd354152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff80d291c70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "V = np.zeros((16,16),dtype=complex)\n",
    "V[1,1] = 1.0 # Should be a low frequency\n",
    "BasisImage = np.fft.ifft2(V)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(BasisImage.real, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(BasisImage.imag, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d02e30-15f1-4861-9f42-de38eb864859",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "    \n",
    "**Exercise 3.4** What is the effect of keeping the magnitude of the non-zero element in transform space 1, but altering the phase?  This exercise should help you understand what the meaning of spatial phase is in terms of the basis images. But the other important thing to note is that discrete Fourier space is an example of what is called an _overcomplete_ representation: there is redundancy built into the transform. &#9724;\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91807e-c784-4d0b-99e3-45f0384a81e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.6 Energy Compaction Property\n",
    "\n",
    "We will now look at the effect of gradually copying the coefficients from the 2D-DFT of the head image into an empty transform space array. The copying process will be done by starting from the biggest coefficient and running to the smaller ones. So, you should put aside your basis images now, and go back to the head image, and it’s associated discrete Fourier\n",
    "space.\n",
    "\n",
    "#### 3.6.1 Sorting array values\n",
    "First, explore the use of the `np.sort()` command. Then, explore the `np.argsort()` command. Essentially, you now want to make sure you understand how to do the following (seemingly pointless) goal:\n",
    "\n",
    "   _Use `np.argsort()`, together with the array you are sorting, in order to   \n",
    "   return the same result as `np.sort()`._\n",
    "\n",
    "**The reason for this will become clear as you proceed to Ex. 3.5**\n",
    "\n",
    "Here are some hints: start by applying both `np.sort()` and `np.argsort()` to a small array of size, say, 3x3. Once you understand how use `np.argsort` to replicate the result of sort, you are ready to proceed.\n",
    "\n",
    "I suggest something like this as a _dummy_ array to get this figured (literally _sorted_ -- sorry :-() out:\n",
    "\n",
    "```python\n",
    "\n",
    "myarray = np.asarray([[9, 7, 5],\n",
    "                     [2, 3, 6],\n",
    "                     [8, 1, 4]])\n",
    "\n",
    "```\n",
    "\n",
    "The next piece of the jigsaw you will need is to figure how to reverse the order of a sort. If you have an array, `arr`, you can reverse its order by indexing syntax: `arr[::-1]`\n",
    "\n",
    "A final element of a hint: you can reshape arrays from `N` dimensions into 1 dimension using `myarr.flatten()`; and you can reshape it in the order and size you wish using `np.ravel()`, or `np.reshape()`.\n",
    "\n",
    "Once you have this little puzzle figured out,  you are equipped for **Section 3.6.2**.\n",
    "\n",
    "If you can't understand what is going on, I suggest speaking with a GTA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ff132-5815-4d82-96d0-94ddd2dcc56d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "    <summary> Click <b> here </b> for one solution to this puzzle... </summary>\n",
    "This is a tough-ish problem! So it is best to take it one step at a time. First, here is a dummy array:\n",
    "\n",
    "```python\n",
    "\n",
    "    myarray = np.asarray([[9, 7, 5],\n",
    "                     [2, 3, 6],\n",
    "                     [8, 1, 4]])\n",
    "```\n",
    "The correct use of `np.sort()` is to ensure that the whole array is taken as one 1D array, otherwise, sorting might be done row by row or column by column. We can do this:\n",
    "\n",
    "`sortedarray = np.sort(myarray, axis=None)`  \n",
    "But this is from smallest values going up. To sort in the other direction (largest to smalles) we would have to do something like this:\n",
    "    \n",
    "`smalltolarge=np.sort(myarray, axis=None)`\n",
    "`largetosmall=smalltolarge[::-1]`\n",
    "\n",
    "To replicate this using `argsort`, proceed as follows:\n",
    "`ind = np.argsort(myarray, axis=None)`\n",
    "`reverseind = ind[::-1]`\n",
    "\n",
    "So, these are the indices into `myarray`, but we now need to apply these indices with `myarray` as a 1D array to get the same output as `sort`:\n",
    "    \n",
    "`mysortedarray = myarray.flatten()[reverseind]`\n",
    "\n",
    "and finally, to make this back into a 2D array, we do the following:\n",
    "    \n",
    "`np.reshape(mysortedarray,(3,3))`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa57e30-5800-4e38-b704-a2c37d982347",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### 3.6.2 Taking subsets of DFT Space\n",
    "\n",
    "In the final part of this practical, we are going to see the effect of retaining only a portion of the coefficients in discrete Fourier space, and assuming the rest to be neglibible (i.e. 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5462f0-9215-4b58-a390-06498febc066",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div style=\"background:#EEEEFF;color:black\">\n",
    "    \n",
    "**Exercise 3.5**\n",
    "    \n",
    "Sort the complex coefficients of the 2D-DFT into decreasing order of _magnitude_. You will need to pay careful attention to the syntax and use of the argsort sort command.\n",
    "\n",
    "Copy the _complex_ coefficients of top 100 _magnitude_ 2D-DFT space of the head image into an (initially) empty matrix, which is denoted `Ydash`, representing a new 2D-DFT space. Apply the inverse 2D-DFT to `Ydash` to generate\n",
    "an approximation `ydash` to the original image `x`. What do you see? \n",
    "\n",
    "You will need to be sorting coefficients according to magnitude, but then copy the correct _complex_ values into the new matrix, so be careful: you are determining sorting order from one array, but using that ordering to find entries into another.\n",
    "\n",
    "Add the next top 100 components in order of magnitude into the `Ydash`, and repeat. Create an animation in which the head image is gradually refined according to the top magnitude coefficients.\n",
    "\n",
    "_Useful tip_:\n",
    "You can clear a previously displayed image with the help of a little utility:\n",
    "\n",
    "`from IPython.display import clear_output`\n",
    "\n",
    "then using \n",
    "\n",
    "`clear_output(wait=True)` \n",
    "\n",
    "after \n",
    "\n",
    "`plt.imshow(ydash)`\n",
    "`plt.show()`\n",
    "\n",
    "should work, provided you are doing this in the Jupyter notebook itself (so, make sure you are not displaying the image in a separate window).\n",
    "  \n",
    "For each group of 100 coefficients, compute the error in reconstruction between the recovered image `ydash` and the original image, `x`. How does the error change with iteration (i.e every 100 coefficients in order to decreasing magnitudes)? Plot a graph of this measure of image reconstruction quality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9d348-40f2-4dfc-9665-d129e934d07b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<details>\n",
    "    <summary> Click <b> here </b> for one solution to <b>Ex 3.5</b>. </summary>\n",
    "\n",
    "```python\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "X = np.fft.fft2(x)\n",
    "Xabs = np.abs(X)\n",
    "ind = np.argsort(Xabs, axis=None)\n",
    "reverseind = ind[::-1]\n",
    "\n",
    "M,N = np.shape(X)\n",
    "Ydash = np.zeros((M,N),dtype=complex)\n",
    "\n",
    "NCoefficients = len(reverseind)\n",
    "\n",
    "# Create some empty list to hold\n",
    "# results as we iterate\n",
    "rmse_L = []\n",
    "percent_coeffs_L = []\n",
    "\n",
    "for i in range(1,NCoefficients,100):\n",
    "    YdashUnravelled = Ydash.flatten()\n",
    "    YdashUnravelled[reverseind[0:i]]=\n",
    "                X.flatten()[reverseind[0:i]]\n",
    "    Ydash=np.reshape(YdashUnravelled,(M,N))\n",
    "    ydash=np.fft.ifft2(Ydash).real\n",
    "    \n",
    "    percentofcoeffs=round(100*float(i)/(M*N))\n",
    "    thisRMSE = np.sqrt(np.mean((ydash - \n",
    "                                   x)**2))\n",
    "    rmse_L.append(thisRMSE)\n",
    "    percent_coeffs_L.append(percent_coeffs)\n",
    "    \n",
    "    plt.imshow(ydash.real)\n",
    "    textstr = '% of coeffs: \n",
    "            '+str(percentofcoeffs) \n",
    "    plt.title(textstr)\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "```\n",
    "\n",
    "At the end of this, you can also plot a graph of the values in the list `rmse_L`, using the following:\n",
    "\n",
    "```python\n",
    "\n",
    "plt.plot(percent_coeffs_L,rmse_L)\n",
    "plt.xlabel('Percentage of coeffs from DFT space')\n",
    "plt.ylabel('Root mean square recon. error')\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1004a7-d11b-4a4d-a218-75c6912ec214",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.7 Learning Outcomes and Comcluding Remarks\n",
    "\n",
    "There are a few key take-aways from this practical:\n",
    "\n",
    "1. You have some basic familarity with using one of the most successful numerical algorithms in science: the discrete Fourier Transform. Based on the Fourier Transform, it is optimised to map a time or spatial sequence into the Discrete Fourier Domain, which is very widely used in science and engineering. This algorithm is also the pillar of image reconstruction for magnetic resonance imaging.\n",
    "\n",
    "2. We have looked at how one can \"extract\" the basis images from a transform given a software library that performs the _inverse_ transform. It's easy: just create an empty transform space of all zeros, and set one value at a time to 1, and comput the inverse transform: what pops out is the associated basis image for that transform, for that coefficient. Repeating this across each coefficient in turn gives the corresponding basis image. A useful hack, but also quite inciteful about the mapping between image and transform space (through Eq A). \n",
    "\n",
    "3. You have had a glimpse into the energy compaction properties of the discrete Fourier transform, and had an introduction to comparing two images using a reconstruction error (ah, so there was a point to that exercise!). We will be returning to this error when we consider autoencoders in Chapter 8, and also will look at it again in image registration (Ch 6)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79ff91-0ff2-4655-9172-a2591001f1a8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Although the 2D DFT does indeed have an energy compaction property, it is **not** practically ideal for compression. Instead, a close relative is, known as the Discrete Cosine Transform. This is almost the real part of the DFT result, with a few other adjustments to give the transform better numerical properties. The key issue with the DFT is that it is _overcomplete_: it *expands* on the number of values in the array, because it uses complex basis functions which are _highly_ redundant. \n",
    "    \n",
    "The redundancy provides some benefit, which is based on the use of the magnitude of the transform to obtain shift-invariance. This will be discussed briefly in the lectures.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
